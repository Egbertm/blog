---
layout: post
title:  "数据挖掘算法（四）naive bayes"
date:   2017-1-07 23:06:05
categories: 算法
tags:  算法 bayes
---
* content  
{:toc}  

### 1 算法的概述

朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。




举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。【1】

### 2 算法的原理

朴素贝叶斯在上世纪50年代得到了广泛的研究，在20世纪60年代就以另外的一个名称被引入到文本检索界中，目前仍然是该领域研究的一个热门的算法[4]。贝叶斯定理是贝叶斯算法的一个数学上的原理支撑。根据该定理，可以已知事件的发生的概率来计算下一次事件发生的可能性。朴素贝叶斯定理公式如下：  
![http://o886hn2n8.bkt.clouddn.com//suanfa/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95.png](http://o886hn2n8.bkt.clouddn.com//suanfa/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95.png)


### 3 算法的代码实现

算法选择对中文垃圾邮件的过滤，垃圾邮件过滤其本质就是实现文本的二元分类的过程，也就是判断一个文本文件是垃圾邮件还是非垃圾邮件的过程其分类过程如图一所示：  
![http://o886hn2n8.bkt.clouddn.com//suanfa/%E8%B4%9D%E5%8F%B6%E6%96%AF2.png](http://o886hn2n8.bkt.clouddn.com//suanfa/%E8%B4%9D%E5%8F%B6%E6%96%AF2.png)  

具体的过程如下：
 每个垃圾邮件文本都可以作为一个文本对象进行处理，设Di表示的是第i个邮件文档。我们要得到文档的数学模型最后生成分类，有以下几步：
1. 需要对文档进行分词处理，处理后的每个文档就又若干个词组成，这些词构成文档Di的属性。因此一篇文档的表示可以是Di(x1,x2,x3,...,xn)。其中n表示文档的维度。  

2. 需要对文档进行特征的值得提取，目前对于特征的提取方法有很多，如有互信息属性提取，TD-IDF属性提取等等。  

3. 所有的文本都表示成了词向量模型，需要对这些词向量进行规格化处理。一般而言我们通过采用词袋模型法进行规格化处理，处理后所有的文本都成了0,1表示的文本向量，这样有个明显的好处就是对于计算机的处理占用的内存更小，处理速度更快。  

4. 就是通过以上几步得到的规格化数据后文本通过贝叶斯分类器进行对测试文本邮件的分类训练得出邮件的分类效果。其中分类器就是帮助我们需要判断Di这个文本是否是属于垃圾邮件或者是非垃圾邮件，因此用分类器的核心算法就是贝叶斯算法，分别计算概率P(Cspam|Di)和P(Cham|Di)表示该文档是属于垃圾邮件还是非垃圾邮件的概率。   
本实验的代码实现是在Windows7平台下，采用的是Eclipse+PyDev的开发环境，开发语言是Python2.7实现了一个例子，代码如下：  

        
        # encoding=utf-8
        import numpy as np
        import jieba
        import os
        import codecs
        from numpy import int8
        '''
        此函数主要是通过给定指定的路径读取文本的内容。
        path 的格式是 D:\\***\***.txt
        '''
        def readfile(path):
            f=open(path,'r')
            return f.read() #f.read().decode("gbk")
        '''
        此函数得到的是文本路径列表
        '''
        def getDoc(rootDir): 
            list_dirs = os.walk(rootDir) 
            docdir=[]
            for root, dirs, files in list_dirs: 
                for f in files: 
                    #print os.path.join(root, f) 
                    docdir.append(os.path.join(root, f))
            return docdir
        #获得停留词之后的文本
        '''
        此函数是获取去掉停留词后的文本doctxt内的内容是Unicode的编码
        是一个List列表
        '''
        def stopWord(text):
            print os.path.join(os.getcwd(),"com/yayin/stopword.dic")
            if os.path.exists(os.path.join(os.getcwd(),"stopword.dic")):
                #print "Using your own stopwords.\n"
                STOP_WORDS = frozenset(( line.rstrip() for line in codecs.open(os.path.join(os.getcwd(),"stopword.dic"),'r','utf-8') ))
            else:
                #print "Using jieba's stopwords.\n"
                STOP_WORDS = frozenset(('a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'can',
                                'for', 'from', 'have', 'if', 'in', 'is', 'it', 'may',
                                'not', 'of', 'on', 'or', 'tbd', 'that', 'the', 'this',
                                'to', 'us', 'we', 'when', 'will', 'with', 'yet',
                                'you', 'your',u'的',u'了',u'和'))
            segs = jieba.cut(text, cut_all=False)  
            final = ''  
            for seg in segs:  
                #seg = seg.encode('utf-8')  
                if seg not in STOP_WORDS:
                    if seg!=u'\n':
                        if seg!=u' ':
                            final += seg  
            #print final
            seg_list = jieba.cut(final, cut_all=False)
            #print STOP_WORDS
            doctxt=[i for i in seg_list]
            return doctxt
        '''
        此函数得到的词典一个词袋 传递的参数是所有的文本分词及去停留词后的结果集
        '''
        def createWordDict(txtset):
            temp=set([])
            for txtitem in txtset:
                temp=temp | set(txtitem)
            return list(temp)
        '''
        此函数得到的是规格化得文本模型里面主要是0,1， listdataset 是 Unicode 编码的文本集
        是一个二维列表，每行是一个文本，wordpackage1 是传进来的词袋，根据词袋生成标准的文本模型
        如[[0,0,1,...,1,0,1],
        [0,0,1,...,1,0,1]
        ]
        '''
        def createStandModel(listdataset,wordpackage1):
            print listdataset
            print len(wordpackage1)
            listdatamodel=[]
            for i in range(0,len(listdataset)):
                listdataitme=[]
                for j in range(0,len(wordpackage1)):
                        if wordpackage1[j] in listdataset[i]:
                            listdataitme.append(1)
                        else:
                            listdataitme.append(0)
                listdatamodel.append(listdataitme)
            return listdatamodel
        '''
        此函数得到是不同类别下，（此例是两类）每个类别的词的总数目
        lables 是类别列表
        txtmodels 是标准的文档集
        wordpackages2 是词袋
        c0totalword 类别0 的数目
        c1totalword 类别1的数目
        '''
        def computeTotalTag(lables,txtmodels,wordpackage2):
            tempclc0=np.zeros((1,len(wordpackage2)),dtype=int8)
            tempclc1=np.zeros((1,len(wordpackage2)),dtype=int8)
            for i in range (0,len(lables)):
                if lables[i]==0:
                    tempclc0=tempclc0+np.array(txtmodels[i])
                else:
                    tempclc1=tempclc1+np.array(txtmodels[i])
            c0totalword=sum(tempclc0[0])
            c1totalword=sum(tempclc1[0])
            return c0totalword,c1totalword
        '''
        此函数得到是测试类别列表，词袋，标准的文档集
        doctxt 是文件名列表
        txtmodel 是标准的文档集
        lablelist 类别列表
        '''
        def getLablelistAndtxtSet(doctxt):
            tempdataset=[]
            lablelist=[]
            for i in range(0,len(doctxt)):
                #print doctxt[i]
                if "ham" in str(doctxt[i]):
                    lablelist.append(0)#0表示非垃圾邮件 1表示垃圾邮件
                else:
                    lablelist.append(1)
                tempsplittxt=stopWord(readfile(doctxt[i]))
                tempdataset.append(tempsplittxt);
            wordpackage=createWordDict(tempdataset);
            #print tempdataset
            txtmodel=createStandModel(tempdataset, wordpackage)
            ff=open("D:\\train.txt",'w')
            ff.write(str(txtmodel))
            return txtmodel,wordpackage,lablelist
        '''
        此函数通过训练的词袋库得到测试类的类别列表，及还回的标准的文档集
        doctxt2 是测试的文件名列表
        wordpackage 是训练的词袋库
        txtmodeltrain 测试文本的标准集
        lablelisttrain 测试文本的类别集
        '''
        def getTrainLableAndtxtset(doctxt2,wordpackage):
            tempdataset=[]
            lablelisttrain=[]
            for i in range(0,len(doctxt2)):
                #print doctxt[i]
                if "ham" in str(doctxt2[i]):
                    lablelisttrain.append(0)#0表示非垃圾邮件 1表示垃圾邮件
                else:
                    lablelisttrain.append(1)
                tempsplittxt=stopWord(readfile(doctxt2[i]))
                tempdataset.append(tempsplittxt);
            #print tempdataset
            txtmodeltrain=createStandModel(tempdataset, wordpackage)
            ff=open("D:\\texttrain.txt",'w')
            ff.write(str(txtmodel))
            return txtmodeltrain,lablelisttrain
        '''
        此函数得到是不同类别中每个词（每列的总数）对应于词袋的总数
        listdataset 是标准文档集
        column 是测试文本中对应与词袋为1的那列[0,1,0]column为第一列0开始
        lableset 训练文本的标签
        sumcolumc0 训练文本0类别，column列的总数
        sumcolumc1 训练文本1类别，column列的总数
        '''
        def computerColumTotalNum(listdataset,column,lableset):
            sumcolumc0=0
            sumcolumc1=0
            for ll in range(0,len(lableset)):
                if lableset[ll]==0:
                    sumcolumc0=sumcolumc0+listdataset[ll][column]
                else:
                    sumcolumc1=sumcolumc1+listdataset[ll][column]
            return sumcolumc0,sumcolumc1
        '''
        此函数计算概率，并判断是否是垃圾邮件
        totalwordc0 训练文本0类别的总数
        totalwordc1 训练文本1类别的总数
        wordpackage3 是词袋
        txt 是测试的标准集
        txtmodel 训练文本标准集
        lableset 类别集
        '''        
        def computeProbability(totalwordc0,totalwordc1,wordpackage3,txt,txtmodel,lableset):
            proc0=0.0
            proc1=0.0
            hams=0
            spans=0
            falseresultspam=0;
            falseresultham=0;
            c0num=0
            c1num=1
            for i in range(0,len(lableset)):
                if lableset[i]==0:
                    c0num+=1
                else:
                    c1num+=1
            pc0=c0num*1.0/len(lableset)
            pc1=c1num*1.0/len(lableset)
            
            for i in range(0,len(txt)):
                for j in range(0,len(txt[i])):
                    if txt[i][j]==1:
                        sumcolumc0,sumcolumc1=computerColumTotalNum(txtmodel, j,lableset)
                        #print (sumcolumc0+1)*1.0/totalwordc0
                        #print (sumcolumc1+1)*1.0/totalwordc0
                        proc0+=np.log2((sumcolumc0+1)*1.0/totalwordc0)
                        proc1+=np.log2((sumcolumc1+1)*1.0/totalwordc1)
                if proc0*pc0>proc1*pc1:
                    hams+=1
                    print "邮件    %d 是非垃圾邮件" %(i+1)
                    if i>199:
                        falseresultspam+=1
                else:
                    spans+=1
                    print "邮件    %d 是垃圾邮件" %(i+1)
                    if i<200:
                        falseresultham+=1;
                        
                proc0=0.0
                proc1=0.0
            return hams, spans,falseresultham,falseresultspam     
        
        if __name__=='__main__':
            PathString="D:\\training set"
            PathString1="D:\\text set"
            #seg_list = jieba.cut("南昌的天气；不错", cut_all=False)
            #print "Full Mode: " + "/ ".join(seg_list)   # 全模式
            #print getDoc("D:\\training set")
            doctxt=getDoc(PathString)
            doctxt2=getDoc(PathString1)
            txtmodel,wordpackage,lablelist=getLablelistAndtxtSet(doctxt)
            #print wordpackage,lablelist
            txtmodeltrain,lablelisttrain=getTrainLableAndtxtset(doctxt2, wordpackage)
            #print txtmodeltrain,lablelisttrain
            c0totalword,c1totalword=computeTotalTag(lablelist, txtmodel, wordpackage)
            hams, spans,falseresultham,falseresultspam =computeProbability(c0totalword, c1totalword, wordpackage, txtmodeltrain, txtmodel, lablelist)
            print "错误率为  %f" % ((falseresultham+falseresultspam)*1.0/float(hams+spans))
            print "召回率为  %f" % ((spans-falseresultham)*2.0/float(hams+spans))
            print "准确率为 %f" % ((spans-falseresultham)*1.0/float(spans))
            print "实验的精度为 %f" % ((hams-falseresultspam+spans-falseresultham)*1.0/float(hams+spans))
    
代码中已经对各个方法进行了介绍，不做过多的介绍。  

### 4 实验与分析

仿真实验的数据来源是CSDN上下载的中文垃圾邮件分类语料里面包含有1400封邮件其中垃圾邮件和非垃圾邮件各有700封，其中各选中200封邮件作为测试集，训练集各有500封邮件。本文通过了实验验证了朴素贝叶斯算法在中文垃圾邮件分类中的有效性，实验结果也显示了该算法能够达到一个不错的效果。目前对于这个实验结果还需要后期的一些认证，因为数据的训练和测试集并不是学术机构提供的专门的测试和训练集。接下来需要做的工作就是找到一份专门的学术机构提供的测试集进行训练和测试认证算法的有效性。当然本次实验本质上还是一个简单的认证过程，还没有涉及到算法的修改，需要在后期对分类器进行一些创新性的修改，最主要的也是在文本属性的选择上，本文没有对垃圾邮件的文本进行属性的约减所以这也是目前存在一个比较大的问题，需要在后期的研究过程中创新。以下是实验得到的结果表格：

|  "  " | 系统判断为垃圾邮件 | 系统判断为合法邮件 |
| ------| ------ | ------ |
| 实际为垃圾邮件 | 184 | 16 |
| 实际为合法邮件 | 4 | 196 |

### 参考文献
【1】[https://zh.wikipedia.org](https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8 "https://zh.wikipedia.org/wiki/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8")  
【2】[http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html](http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html "http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html")  